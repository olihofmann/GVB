{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import glob\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "import re\n",
    "\n",
    "with open(\"Schadenshergang.csv\", \"a\") as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=\";\")\n",
    "    writer.writerow([\"Schadenshergang\"])\n",
    "    for f in glob.iglob(\"../Inputs/Schadenmeldungen/*.txt\"):\n",
    "        with open(f) as file:\n",
    "            text = file.read()\n",
    "            m = re.search(\"(?<=Schadenshergang:).*\", text)\n",
    "            writer.writerow([m.group(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121, 1)\n",
      " Der Knetkacken ist mir beim Abtrocknen runter auf die Glaskeramikplatte gefallen.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Schadenshergang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Der Knetkacken ist mir beim Abtrocknen runter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Habe am 31. Juli auf der Terrasse Dachisolati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Die Fäkalienpumpe im UG stellt nicht mehr ab ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beim Unwetter wurde Lamellenstore beschädigt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unbekannt, wir vermuten als Ursache einer der...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Schadenshergang\n",
       "0   Der Knetkacken ist mir beim Abtrocknen runter...\n",
       "1   Habe am 31. Juli auf der Terrasse Dachisolati...\n",
       "2   Die Fäkalienpumpe im UG stellt nicht mehr ab ...\n",
       "3       Beim Unwetter wurde Lamellenstore beschädigt\n",
       "4   unbekannt, wir vermuten als Ursache einer der..."
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Schadenshergang.csv\", delimiter=\";\", header=0)\n",
    "print(data.shape)\n",
    "print(data[\"Schadenshergang\"].iloc[0])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "    return [w.lower() for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('german')\n",
    "    wordlist_stripped = [w for w in text if w.lower() not in stopwords]\n",
    "    return wordlist_stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(wordlist):\n",
    "    punctuation = ['.', ',', ';', ':', '(', ')', '[', ']', '{', '}', '\\\"', '\\'','\\'\\'', '\\`', '\\`\\`', '==', '===', '====', '-']\n",
    "    wordlist_stripped = [w for w in wordlist if w not in punctuation]\n",
    "    wordlist_stripped = [w for w in wordlist if len(w) > 2]\n",
    "    return wordlist_stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list_from_schadenmeldung(text):\n",
    "    print(text)\n",
    "    wordlist = nltk.word_tokenize(text)\n",
    "    wordlist = lowercase(wordlist)\n",
    "    wordlist = remove_stopwords(wordlist)\n",
    "    wordlist = remove_punctuation(wordlist)\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Knetkacken', 'Abtrocknen', 'Glaskeramikplatte', 'Habe', 'Juli', 'Terrasse', 'Dachisolationsfetzen', 'Eingang', 'Marder', 'Fäkalienpumpe', 'Toilette', 'Studio', 'Wasserlache', 'Gerät', 'Fa.', 'AG', 'Sanitär', 'Sanitätshop.de', 'Unwetter', 'Lamellenstore', 'Ursache', 'Winterstürme', 'Datum', 'Schätzung', 'Fassade', 'Liegenschaft', 'Graffiti', 'Feuerwerkskörper', 'Sonnenstore', 'Sturm', 'Ziegel', 'Gewitter', 'Ortladen', 'Aushärtungsphase', 'Futtertischbeschichtung', 'Wassereinbruch', 'Wassereintritt', 'Waschküche', 'Schimmel', 'Keller', 'Brand', 'Umbau', 'Sanierung', 'Saal', 'Pflastermulde', 'Holz', 'Feuer', 'Sturmwind', 'Welleternit-Dach', 'Hagelereignis', 'Blumentrog', 'Parkplatz', 'Dienstag', 'Sintflutartigen', 'Regenfällen', 'Teil', 'Kellers', 'Dachdecker', 'Situation', 'Dachkanals', 'Rohrreinigungsfirma', 'Kanal', 'Dachwassers', 'Firma', 'Flückiger', 'Nachmittag', 'Wunsch', 'Ablauf', 'Leitung', 'Tagen', 'Kamera', 'Schäden', 'Kamera-Aufzeichnung', '31.07.2018', 'Termin', '31.07.18', 'Geschäftsführer', 'Offerte', '\\\\\"Rohr', 'Kostenbeteiligung', 'Sturmbö', 'Knickarm', 'Simme', 'Gletschersees', 'Ufer', 'Rückstau', 'Fäkalien', 'Wespennest', 'Regen', 'Abläufe', 'Hinterhof', 'Nachbarliegenschaft', 'Fenster', 'Sofortmassnahme', 'Auspumpen/Trocknen', 'Fall', 'Schadennummer', '/', 'Korrespondenz', 'Seite', 'Anschluss', 'Reparatur', 'Gelenkarmes', 'Motor', 'Schreiben', 'Schaden', 'Grosses', 'Kosten', 'Entfernung', 'Feuerwehr', 'Sonnenstoren', 'Loch', 'Störungsmeldung', 'Service-Techniker', 'Komponente', 'Grund', 'Einflüsse', 'Blitzschlag', 'Spreyereien', 'Dach', 'Leider', 'Ball', 'Kindern', 'Haustüre', 'Türglas', 'Hochwasser', 'Wochenende', 'Teile', 'Grundstücks', 'Grundwasser-Pumpenschächte', 'Schlamm', 'Wind', 'Hagel', 'Westfassade', 'Beschädigung', 'Garten/Rasen', 'Wildtiere', 'Wasser', 'Lukarnen', 'Unterspülung', 'Unbekannte', 'Sommergewitter', 'Decke', 'Küche', 'OG', 'Flecken', 'Gewerberaum', 'Elefant', 'GmbH.', 'Mieter', 'Küchenablauf', 'Verschraubungen', 'Wasseraustritt', 'Undichter', 'Folgeschaden', 'Tielkühlschrank', 'Einschussloch', 'Polizeirapport', 'Niederschläge', 'Dachwasser', 'Sickerleitung', 'Kontrollschacht', 'Dohlendeckel', 'Treppenpodest', 'Glasbruch', 'Rückstaus', 'Kanalisation', 'Sanitärinstallateur', 'Umgebung', 'Kanalreinigung', 'Schleudergang', 'Waschmaschine', 'Lärm', 'Fehlermeldung', 'Aufgebot', 'Technikers', 'Morgen', 'Windböen', 'Überschwemmungen', 'Quartier', 'Ameisenbefall', 'Zimmer:3206', 'Nachbarn', 'Fam.', 'Wüthrich', 'Blitz', 'Wand', 'Fussleiste', 'Badzimmer', 'Steckdichtung', 'Sifonanschlusswinkel', 'Einstecktiefe', 'August', 'Storen', 'Letzten', 'Rubigen', 'Grill', 'Frau', 'Balkon', 'Bodenplatten', 'Fassaden', 'Toilettenanlage', 'Zeitraum', 'Kamin', 'Glasgefäss', 'Spiegelschrank', 'Keramik-Waschtisch', 'Zeitpunkt', 'Minuten', 'Händewaschen', 'Sprung/Spalt', 'Waschtisch', 'Heizung', 'Elektronik', 'Überschwemmung', 'Glasscheibe', 'Wohnzimmer', 'Sprung', 'Spannung', 'Sonne', 'Säriswil', 'Hagelgewitter', 'Sturmböhen', 'Abwesenheit', 'Sensoren', 'Sturms', 'Store', 'Wassersack', 'Querstrebe', 'Konstruktion', 'Dachziegel', 'Chemineerohrdurchgang', 'Stores', '\\\\\"Chrischichracher\\\\', 'Briefkasten', 'Defektes', 'Form', 'Wassermehrverbrauch', 'Wohnküche', 'Wasserleitung', 'Spülmaschine', 'Parkett', 'Nacht', 'Bereich', '-etwa', 'Grösse', 'Risse', 'Pflegemassnahmen', 'Undichte', 'Stelle', 'Bade', 'Wohnungsabnahme', 'WC', 'Fleck', 'Messungen', 'Feuchtigkeit', 'Regenwasser', '\\\\\"RohrMax\\\\', 'Luftschutzkeller', 'Unwetter-Dreck', 'Mobiliar', 'Elektrokurzschluss', 'Flipperkasten', 'Schwelbrand', 'Rauchmelder', 'Ort', 'Mobilien', 'Türen', 'Windböe', 'Temperaturen', 'Regenzelle', 'Stoff', 'Teleskoparme', 'Ferien', 'Laden', 'Gelaufen', 'Auslaufen', 'Flüssigkeit', 'Heizungskessel', 'Hagelschlag', 'Gewittersturm', 'Abdeckung', 'Gebäude', 'Pavatex-Decke', 'Blitzeinschläge', 'Nähe', 'Knall', 'Sekunde', 'Objekte', 'Sporthallen-Gelände', 'Fensterstore', 'Hagelschaden', 'Dato', 'Eiergrosse', 'Niedergang', 'Jahr', 'Elektroleitungen', 'Rohre', 'Rohrbruch', 'Herr', 'Wespennester', 'Tel.', 'Nr.', 'Mieters', 'Rinnstelle', 'Infos', 'Dachkänel', 'Haus', 'Rohrreiniger', 'Lavabo', 'Küchensanierung', 'Richtung', 'Badezimmer', 'Leck', 'Badewanne', 'Burglind', 'Windstösse', 'Scheunendach', 'Kurzzeitige', 'Zerstörung', 'Kellerräumlichkeiten', 'Abwasserschacht', 'Menge', 'Sturmböen', 'Region', 'Sonennstore', 'Partei', 'Gestänge', 'Wintergartens', 'Familie', 'Einschlagstelle', 'Frost', 'Bersten', 'Glases', 'Wasserunterbruch', 'Glasdach', 'Brunner', 'Holzbretter', 'Zivilschutzraum', 'Einstellhalle', 'Verglasung', 'Mehrfamilienhaus', 'Brett', 'Stromunterbruch', 'Belästigung', 'Insekten', 'Fotos', 'Anhang', 'Wasserleitungsbruch', 'Zuleitung']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"de\")\n",
    "words = []\n",
    "for k, v in data.iterrows():\n",
    "    doc = nlp(v[\"Schadenshergang\"])\n",
    "    for t in doc:\n",
    "        # print(t.orth_ + \" \" + t.pos_ + \" \" + str(t.head.is_stop))\n",
    "        if t.pos_ == \"NOUN\" and t.orth_ not in words:\n",
    "            words.append(t.orth_)\n",
    "            \n",
    "            \n",
    "pd.DataFrame(words).head()\n",
    "print(words)\n",
    "                \n",
    "with open(\"Corpus/gvb_corpus.txt\", \"a\") as textFile:\n",
    "    for w in words:\n",
    "        textFile.writelines(w + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvb_korpus = nltk.corpus.PlaintextCorpusReader(\"Corpus/\", \".*\")\n",
    "words = nltk.FreqDist(gvb_korpus.words())\n",
    "count = sum(words.values())\n",
    "vocab = len(words)\n",
    "\n",
    "print(count)\n",
    "print(vocab)\n",
    "\n",
    "gvb_word_list = [w for w in gvb_korpus.words()]\n",
    "matches = []\n",
    "for w in gvb_word_list:\n",
    "    if w in data[\"Schadenshergang\"].iloc[1] and w not in matches:\n",
    "        matches.append(w)\n",
    "        \n",
    "for m in matches:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Schadenshergang_categorize.csv\", \"a\") as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=\";\")\n",
    "    writer.writerow([\"Schadenshergang\", \"Objekte\", \"Schadensursache\"])\n",
    "    for f in glob.iglob(\"../Inputs/Schadenmeldungen/*.txt\"):\n",
    "        with open(f) as file:\n",
    "            text = file.read()\n",
    "            m_hergang = re.search(\"(?<=Schadenshergang:).*\", text)\n",
    "            m_object_geraete = re.search(\"(?<=Beschädigte Geräte:).*\", text)\n",
    "            m_object_umgebung = re.search(\"(?<=Beschädigte Umgebung:).*\", text)\n",
    "            m_object_gebaeude = re.search(\"(?<=Beschädigte Gebäudeteile:).*\", text)\n",
    "            m_ursache = re.search(\"(?<=Schadensursache:).*\", text)\n",
    "            writer.writerow([m_hergang.group(0).lstrip(), m_object_geraete.group(0).lstrip() + \" \" + m_object_umgebung.group(0).lstrip() + \" \" + m_object_gebaeude.group(0).lstrip(), m_ursache.group(0).lstrip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_cat = pd.read_csv(\"Schadenshergang_categorize.csv\", delimiter=\";\", header=0)\n",
    "csv_cat = csv_cat.dropna()\n",
    "X = csv_cat.drop(columns=[\"Schadensursache\"])\n",
    "y = csv_cat[\"Schadensursache\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"german\")\n",
    "stopwords = nltk.corpus.stopwords.words('german')\n",
    "def preprocess_text(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word not in stopwords]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=preprocess_text)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(csv_cat[\"Schadenshergang\"] + \" \" + csv_cat[\"Objekte\"])\n",
    "print(tfidf_matrix.shape)\n",
    "print(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_metric(data, true_labels, cluster_labels):\n",
    "    print()\n",
    "    print(\"Cluster Metric:\")\n",
    "    print(\"Homogeneity: \", metrics.homogeneity_score(true_labels, cluster_labels))\n",
    "    print(\"Completeness: \", metrics.completeness_score(true_labels, cluster_labels))\n",
    "    print(\"V-Measure: \", metrics.v_measure_score(true_labels, cluster_labels))\n",
    "    print(\"Silhouette Score: \", metrics.silhouette_score(data, cluster_labels, metric='euclidean'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_k_means(data, k_means):\n",
    "    plt.figure(figsize=(8,6), dpi=120)\n",
    "    k = len(k_means.cluster_centers_)\n",
    "    for i in range(k):\n",
    "        colors = ['r', 'g', 'b', 'y', 'c', 'm']\n",
    "        points_in_cluster = np.array([data[j] for j in range(len(data)) if k_means.labels_[j] == i])\n",
    "        plt.scatter(points_in_cluster[:, 0], points_in_cluster[:, 1], \\\n",
    "                    marker='.', color=colors[i % len(colors)])\n",
    "    plt.scatter(k_means.cluster_centers_[:,0], k_means.cluster_centers_[:, 1], marker='*', color='k')\n",
    "    plt.title('k-Means Clustering with k={} clusters'.format(k))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 25\n",
    "k_means = KMeans(n_clusters=k, max_iter=100, n_init=10, init=\"k-means++\").fit(tfidf_matrix)\n",
    "clusters = k_means.labels_.tolist()\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "order_centroids = k_means.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(k):\n",
    "    print(\"Cluster {}\".format(i))\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(terms[ind])\n",
    "    print()\n",
    "    \n",
    "cluster_metric(tfidf_matrix, y, k_means.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = SparsePCA(n_components=2)\n",
    "tfidf_matrix_pca = pca.fit_transform(tfidf_matrix.toarray())\n",
    "print(tfidf_matrix_pca.shape)\n",
    "\n",
    "k = 25\n",
    "k_means = KMeans(n_clusters=k, max_iter=100, n_init=10, init=\"k-means++\").fit(tfidf_matrix_pca)\n",
    "cluster_metric(tfidf_matrix_pca, labels, k_means.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_cat = pd.read_csv(\"Schadenshergang_categorize.csv\", delimiter=\";\", header=0)\n",
    "X = csv_cat.drop(columns=[\"Schadensursache\"])\n",
    "y = csv_cat[\"Schadensursache\"]\n",
    "\n",
    "test = []\n",
    "for k, v in X.iterrows():\n",
    "    test.append(v[\"Schadenshergang\"] + \" \" + v[\"Objekte\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(test, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,2))),\n",
    " (\"tfidf\", TfidfTransformer(use_idf=True)),\n",
    " (\"clf\", OneVsRestClassifier(LinearRegression()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 45.946%\n",
      "\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "                          Blitzschlag       1.00      0.50      0.67         2\n",
      "                                Feuer       0.00      0.00      0.00         2\n",
      "              Gebäudehaftpflichtsfall       0.00      0.00      0.00         1\n",
      "                       Gebäudetechnik       0.00      0.00      0.00         3\n",
      "                            Glasbruch       0.00      0.00      0.00         0\n",
      "                    Grund-/Hangwasser       0.00      0.00      0.00         1\n",
      "                                Hagel       0.75      0.75      0.75         4\n",
      "                           Hochwasser       0.00      0.00      0.00         1\n",
      "                        Leitungsbruch       0.33      0.33      0.33         3\n",
      " Marder-, Nager- oder Insektenschäden       0.50      0.50      0.50         4\n",
      "         Regen-/Schnee-/Schmelzwasser       0.00      0.00      0.00         1\n",
      "                            Sturmwind       0.56      0.83      0.67         6\n",
      "                          Vandalismus       0.44      0.80      0.57         5\n",
      "                     Wasser: Rückstau       0.00      0.00      0.00         1\n",
      "elektrische Überspannung bei Gewitter       0.00      0.00      0.00         0\n",
      "                       Überschwemmung       1.00      0.33      0.50         3\n",
      "\n",
      "                          avg / total       0.45      0.46      0.42        37\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliver.hofmann/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/oliver.hofmann/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "confusion_matrix(pred, y_test)\n",
    "print(\"Test Score: {:.3%}\\n\".format(model.score(X_test, y_test)))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export the model to model.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"Export the model to model.pkl\")\n",
    "f = open('model.pkl', 'wb')\n",
    "pickle.dump(model, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import the model from model.pkl\n"
     ]
    }
   ],
   "source": [
    "print('Import the model from model.pkl')\n",
    "f2 = open('model.pkl', 'rb')\n",
    "pickel_model = pickle.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sturmwind'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickel_model.predict([\"Als der Orkan Lothar, welcher über der Biskaya enstanden ist und mit einer spitzen Geschwindigkeit von 272km/h über die Schweiz fegte, fegte er auch über mein zu Hause! Dabei entwurzelte der Sturm Hecken aus dem Garten, herumfliegende Äste haben mehrere Fenster eingeschlagen, sinnflutartige Regenfälle haben meinen Keller gefüllt und Wasserschaden verursacht und eine kräftigen Windböen haben die Ziegel vom Dach gerissen.\"])[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
